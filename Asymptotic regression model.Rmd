---
title: "Diana's Version VRI"
author: "Sylvia Liu"
date: "20/05/2022"
output: html_document
---

```{r}
library(tidyverse)
library(dplyr)
library(readxl)
library(tibble)
library(ggplot2)
library(ggthemes)
library(maps)

library(plotly) # Interactive data visualizations
library(viridis) # Color gradients
library(lubridate)

library(randomForest) 
library(ranger)       # a faster implementation of randomForest
library(caret)        # performe many machine learning models
library(broom)  # try: `install.packages("backports")` if diretly installing broom gives an error

library(reshape2) # to use melt()

library(deSolve)
library(permimp) # for calculating conditional importance in a speedy way
```

## Reading data
```{r}
covid_data <- read.csv("owid-covid-data.csv")
covid_data$date <- as.Date(covid_data$date) #, format = "%d/%m/%y")
covid <- covid_data
#head(covid_data)
```


## Vaccinations progress  

### Vaccine Roll-Out Index (vri) values for the countries

(Reference: https://www.mdpi.com/2076-393X/10/2/194/htm)

> **Estimate Vaccine Uptakes Rates r* for Countries**


```{r}
## NEW
covid_clean = covid_data %>% 
  filter(str_length(iso_code) <= 3) %>% 
  select(iso_code, location, date, people_vaccinated,population) %>% 
  remove_missing() %>%
  filter(people_vaccinated != 0) %>% 
  group_by(location) %>% 
  mutate(t_days = difftime(date, min(date), units = "days") %>% as.integer())
```

```{r}
## calculate vri using: est_rate * d/N
estimate_vri <- function(data, r){
  vri <- r * (data$people_vaccinated[length(data$people_vaccinated)]) / data$population[1]
  
  return(vri)
}
```


```{r logy-asympreg}
# initiate r1_list, to store location, r, fit, respectively
r1_list = list("location" = c(), "r" = c(), "fit" = c())
vri_data <- NULL

#covid_clean$location

## for each country (location)
for (loc in unique(covid_clean$location)) {
  # subset cleaned data to one country
  covid_subset = covid_clean %>% filter(location == loc)
  
  # try to fit model
  fit = tryCatch(
    # main function, fit asymptote regression model
    expr = {
      nls(log(people_vaccinated) ~ SSasymp(t_days, Asym, R0, lrc), data = covid_subset)
    },
    # if error, fit linear model
    error = function(error_message){
      return( lm(log(people_vaccinated) ~ t_days, data = covid_subset) )
    }
  )
  
  # calculate my r value, may need transform before put into vri formula
  if (class(fit) == "lm") {
    r = coef(fit)["t_days"]
  } else if (class(fit) == "nls") {
    lrc = coef(fit)["lrc"]
    r = exp(lrc)
  }
  
  r1_list$location = c(r1_list$location, loc)
  r1_list$r = c(r1_list$r, r)
  r1_list$fit = c(r1_list$fit, list(fit))
  
  vri <- estimate_vri(covid_clean, r)
  
  
  temp <- data.frame(location = loc, vri = vri,
                      aged_65_older = covid[covid$location == loc, ]$aged_65_older[1],
                      gdp_per_capita = covid[covid$location == loc, ]$gdp_per_capita[1],
                      cardiovasc_death_rate = covid[covid$location == loc, ]$cardiovasc_death_rate[1],
                      population_density = covid[covid$location == loc, ]$population_density[1],
                      hospital_beds_per_thousand = covid[covid$location == loc,]$hospital_beds_per_thousand[1],
                      human_development_index = covid[covid$location == loc, ]$human_development_index[1],
                      extreme_poverty = covid[covid$location == loc, ]$extreme_poverty[1],
                      diabetes_prevalence = covid[covid$location == loc, ]$diabetes_prevalence[1],
                      life_expectancy = covid[covid$location == loc, ]$life_expectancy[1],
                      iso_code = covid[covid$location == loc, ]$iso_code[1])


  vri_data <- rbind(vri_data, temp)
}


## Measuring model fitness, residual standard error (RSE).
for (i in seq_len(length(r1_list$fit))) {
  r1_list$rse[[i]] = sigma(r1_list$fit[[i]])
}


## Country using linear model 
unwanted_loc <- r1_list$location[names(r1_list$r) == "t_days"]
vri_data <- vri_data %>% filter (!(location %in% unwanted_loc))
```

```{r}
head(vri_data)
```



#### Merging datasets

**I have comment out the government trust dataset as it only contains OECD countries**


Merge joint covid data with vri.

```{r}
corruption <- read_xlsx("CPI2020_GlobalTablesTS_210125.xlsx", sheet=1,skip=2)
corruption <- corruption %>% select (c("ISO3", "CPI score 2020")) 
#corruption

happiness <- read_csv("happiness-cantril-ladder.csv")
happiness <- happiness %>% group_by(Code) %>%
  arrange(Year) %>%
  filter(!is.na(`Life satisfaction in Cantril Ladder (World Happiness Report 2021)`)) %>%
  dplyr::summarise(satisfaction=last(`Life satisfaction in Cantril Ladder (World Happiness Report 2021)`)) 


joint_data <- merge(vri_data,corruption,  by.x=c("iso_code"), by.y=c("ISO3"))

joint_data <- merge(joint_data,happiness, by.x=c("iso_code"), by.y=c("Code"))

# GHE-INDEX
ghs <- read_csv("2021-GHS-Index-April-2022.csv")
ghs <- ghs %>% filter(Year == 2021) %>% select(Country,`OVERALL SCORE`)

joint_data <- merge(joint_data, ghs,  by.x=c("location"), by.y=c("Country"))
colnames(joint_data)[15] <- "GHS_score"
joint_data
```

#### Selecting Covariates

```{r}
# Basic scatter plot
df <- joint_data %>% filter(  !is.na(population_density) # set a threshold to remove outliers:(vri < 0.08) &
                                    &!is.na(gdp_per_capita) &!is.na(aged_65_older)&
                                      !is.na(extreme_poverty)&
                                      !is.na(human_development_index)& 
                                      !is.na(cardiovasc_death_rate)& 
                                      !is.na(diabetes_prevalence)&
                                      !is.na(hospital_beds_per_thousand)) %>% 
                                      
                            select(c(location,vri,population_density,gdp_per_capita,aged_65_older,
                                     `CPI score 2020`,
                                     satisfaction,extreme_poverty,human_development_index,life_expectancy,
                                      cardiovasc_death_rate,diabetes_prevalence,
                                      hospital_beds_per_thousand, GHS_score)) 

df
```

```{r}
## RUN THIS
df <- df %>% filter(vri < 0.01)
```

##### Correlation Check
```{r}
# correlation matrix

cor_mat <- round(cor(df %>% select(-location), method="spearman"),2)# for non-linear correlation (non-parametric)

## Reorder the correlation matrix
# Refernce: http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

reorder_cormat <- function(cor_mat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cor_mat)/2)
  hc <- hclust(dd)
  return (cor_mat[hc$order, hc$order])
}

```


```{r}
## Plotting heatmap
plot_heatmap <- function(cor_mat){
  
  cor_mat <- reorder_cormat(cor_mat)

  ## Remove redundant information (taking the upper triangles values)
  cor_mat[lower.tri(cor_mat)]<- NA
  cor_mat <- melt(cor_mat, na.rm = TRUE)
  
  ggheatmap <- ggplot(cor_mat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Spearman\nCorrelation") +
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 8, hjust = 1))+
 coord_fixed()

  ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 3) +
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

}

plot_heatmap(cor_mat)

```


#### Modelling (RF)

##### Pre-processing
```{r}
#df
df <- df %>% select(-location)
min_max_norm <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}
#df
scaled_data <- data.frame(lapply(df[2:13], min_max_norm), vri = df$vri)
# UNSURE RN
scaled_data
```

##### Split data into training and testing
```{r}
set.seed(3888)
ind <- createDataPartition(scaled_data$vri, p = .8,list=FALSE) 
train <- scaled_data[ind,]
X_test <- scaled_data[-ind,] %>% select(-vri)
y_test <- scaled_data[-ind,] %>% select(vri)
dim(train)
dim(X_test)
train

```


##### Tune models on training data with all predictors using grid search with cross-validation
```{r warning=FALSE}
# hyper parameter grid search (definitely need a bit modify)
mtry <-  seq(3, 11, by = 1)
num_trees <- c(100,150,200,250,300,350,400,450,500)


# Manual Search
#control <- trainControl(method="cv", number=3, search="grid")
grid_param <- expand.grid(.mtry=mtry)
modellist <- list()
for (ntree in num_trees) {
	set.seed(3888)
	fit <- train( vri~., 
                      data= scaled_data, #train
                      method='rf', 
                      tuneGrid=grid_param, 
	                    ntree= ntree,
                      trControl=trainControl(method='cv', 
                        number=3) )
	key <- toString(ntree)
	modellist[[key]] <- fit$finalModel

}


## COMPARE RESULTS

#lowest_mse <- 1
#model_mse <- modellist[[1]]
highest_r2 <- 0
model_r2 <- modellist[[1]]
for (i in c(1:length(modellist))) {

  result <- predict(modellist[[i]], newdata=train %>% select(-vri))
  result_avg <- mean(train$vri)
  r2 = 1 - (sum((train$vri - result)^2))/(sum((train$vri - result_avg)^2))
  if (highest_r2 < r2){
     highest_r2 = r2
     model_r2 = modellist[[i]]
  }
  # mse = min(modellist[[i]]$mse)
  # if (lowest_mse > mse){
  #   lowest_rmse = mse
  #   model_mse = modellist[[i]]
  # }
 
}
#model_mse
model_r2 
highest_r2

```
